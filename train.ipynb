{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Dense, Lambda\n",
    "from keras import backend as K\n",
    "import tqdm\n",
    "\n",
    "def get_data(data_file, batch_size, length):\n",
    "  inds = np.load(data_file)[:, :length]\n",
    "  inds[inds==0] = ord('\\t')\n",
    "  start = ord('\\t') * np.ones((inds.shape[0], 1))\n",
    "  inds = np.concatenate([start, inds], axis=-1).astype(np.int32)\n",
    "  length += 1\n",
    "  vocab_size = inds.max() + 1\n",
    "  I = np.eye(vocab_size, dtype=np.float32)\n",
    "  def sample():\n",
    "    i = np.random.randint(inds.shape[0])\n",
    "    Xi = I[inds[i]]\n",
    "    return Xi\n",
    "  X = tf.py_func(sample, [], [tf.float32])[0]\n",
    "  X.set_shape((length, vocab_size))\n",
    "  batch = tf.train.shuffle_batch(\n",
    "    [X], batch_size,\n",
    "    capacity=batch_size*10,\n",
    "    min_after_dequeue=0,\n",
    "    num_threads=1)\n",
    "  return batch\n",
    "\n",
    "def Sample():\n",
    "  def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "  return Lambda(sampling, name='sample')\n",
    "\n",
    "def encoder(sequence, latent_dim):\n",
    "  seq = Input(tensor=sequence)\n",
    "  _, h = GRU(latent_dim, return_state=True, activation='tanh')(seq)\n",
    "  mu, logsig = [Dense(latent_dim)(h) for _ in (0, 1)]\n",
    "  z = Sample()([mu, logsig])\n",
    "  return Model(inputs=seq, outputs=[z, mu, logsig])\n",
    "\n",
    "def decoder(sequence, state, latent_dim):\n",
    "  vocab_size = int(sequence.shape[-1])\n",
    "  seq = Input(tensor=sequence)\n",
    "  state = Input(tensor=state)\n",
    "  z = Dense(latent_dim)(state)\n",
    "  x, _ = GRU(latent_dim, return_sequences=True, return_state=True, \n",
    "    activation='tanh')(seq, initial_state=z)\n",
    "  x = Dense(vocab_size)(x)\n",
    "  return Model(inputs=[seq, state], outputs=x)\n",
    "\n",
    "def Switch():\n",
    "  def cond(args):\n",
    "    A, B = args\n",
    "    return tf.cond(K.learning_phase(), lambda: A, lambda: B)\n",
    "  return Lambda(cond, name='switch')\n",
    "\n",
    "def recurrent_VAE(sequence, latent_dim):\n",
    "  enc = encoder(sequence, latent_dim)\n",
    "  state, mu, logsig = enc.outputs\n",
    "  # keras Model.__call__(X) connects model.outputs to given X, but does not \n",
    "  # connect state variabels (such as GRU init. state and batchnorm params) to X, \n",
    "  # so we explicitly build the model with a \"switch\" input that uses placeholder\n",
    "  # inputs when learning_phase == False (but alas, also always expects dummy PHs)\n",
    "  sequence_ph = Input(shape=(None, int(sequence.shape[-1])))\n",
    "  state_ph = Input(shape=state.shape[1:])\n",
    "  placeholders = [sequence_ph, state_ph]\n",
    "  dec_seq = Switch()([sequence, sequence_ph])\n",
    "  dec_state = Switch()([state, state_ph])\n",
    "  dec = decoder(dec_seq, dec_state, latent_dim)\n",
    "  return Model(inputs=enc.inputs+dec.inputs+placeholders, \n",
    "               outputs=enc.outputs+dec.outputs)\n",
    "\n",
    "def xentropy(X, Xhat):\n",
    "  xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=tf.argmax(X, axis=-1), logits=Xhat)\n",
    "  return tf.reduce_mean(xent)\n",
    "\n",
    "def KL_divergence(mu, logsig):\n",
    "  KL = 1 + logsig - mu**2 - tf.exp(logsig)\n",
    "  return -tf.reduce_mean(KL)\n",
    "\n",
    "def regularize(weights):\n",
    "  return tf.global_norm(weights) / len(weights)\n",
    "\n",
    "def matrix_to_string(m):\n",
    "  return ''.join([chr(i) for i in np.argmax(m, axis=-1)])\n",
    "\n",
    "def test(sess, Xhat, state, sequence, desired_length):\n",
    "  vocab_size = int(sequence.shape[-1])\n",
    "  latent_dim = int(state.shape[-1])\n",
    "  I = np.eye(vocab_size)\n",
    "  seed = I[[ord('\\t')]]\n",
    "  init_state = np.random.normal(size=latent_dim)\n",
    "  for i in range(desired_length):\n",
    "    feed_dict = {\n",
    "      K.learning_phase(): False,\n",
    "      sequence: [seed],\n",
    "      state: [init_state]}\n",
    "    result = sess.run(Xhat, feed_dict=feed_dict)\n",
    "    new_char = I[[result[0, -1].argmax()]]\n",
    "    seed = np.concatenate((seed, new_char))\n",
    "  return matrix_to_string(seed)\n",
    "  \n",
    "def train(model):\n",
    "  X = model.inputs[0]\n",
    "  X_shifted = tf.manip.roll(X, shift=-1, axis=1)\n",
    "  sequence_ph, state_ph = model.inputs[-2:]\n",
    "  state, mu, logsig, Xhat = model.outputs\n",
    "  print('got data')\n",
    "  XE = xentropy(X_shifted, Xhat)\n",
    "  KL = KL_divergence(mu, logsig)\n",
    "  RG = 1e-4*regularize(model.trainable_weights)\n",
    "  L = XE + KL + RG\n",
    "  print('got objective')\n",
    "  adam = tf.train.AdamOptimizer()\n",
    "  grad, var = zip(*adam.compute_gradients(\n",
    "    L, var_list=model.trainable_weights))\n",
    "  grad, norm = tf.clip_by_global_norm(grad, clip_norm=5)\n",
    "  opt = adam.apply_gradients(zip(grad, var)) \n",
    "  print('got optimizer')\n",
    "  dummy_dict = {\n",
    "    K.learning_phase(): True,\n",
    "    sequence_ph: np.zeros((1, 1, int(sequence_ph.shape[-1]))),\n",
    "    state_ph: np.zeros((1, int(state_ph.shape[-1])))}\n",
    "  coord = tf.train.Coordinator()\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    print('initialized variables')\n",
    "    while 1:\n",
    "      losses = []\n",
    "      print(flush=True)\n",
    "      for i in tqdm.trange(1000):\n",
    "        losses.append(sess.run([opt, XE, KL, RG, norm], feed_dict=dummy_dict)[1:])\n",
    "      print(flush=True)\n",
    "      print('xent: {}\\nKL:   {}\\nRG:   {}\\nnorm: {}'.format(*np.mean(losses, axis=0)))\n",
    "      print()\n",
    "      print(test(sess, Xhat, state_ph, sequence_ph, int(X.shape[1])-1))\n",
    "\n",
    "batch_size = 1024\n",
    "max_length = 64\n",
    "latent_dim = 512\n",
    "\n",
    "X = get_data('./tweets.npy', batch_size, max_length)\n",
    "model = recurrent_VAE(X, latent_dim)\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
