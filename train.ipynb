{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, CuDNNGRU, Conv1D, Activation\n",
    "from keras import backend as K\n",
    "import glob\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "def LogNormal():\n",
    "  def func(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "  return Lambda(func, name='log_normal')\n",
    "\n",
    "def encoder(sequence, latent_dim):\n",
    "  seq = Input(tensor=sequence)\n",
    "  x = Conv1D(64, 9, activation='relu', padding='same')(seq)\n",
    "  _, h = CuDNNGRU(latent_dim, return_state=True)(x)\n",
    "  mu, logsig = [Dense(latent_dim)(h) for _ in (0, 1)]\n",
    "  z = LogNormal()([mu, logsig])\n",
    "  return Model(inputs=seq, outputs=[z, mu, logsig])\n",
    "\n",
    "def decoder(sequence, state, latent_dim):\n",
    "  vocab_size = int(sequence.shape[-1])\n",
    "  seq = Input(tensor=sequence)\n",
    "  state = Input(tensor=state)\n",
    "  x, _ = CuDNNGRU(latent_dim, return_sequences=True, return_state=True)(seq, initial_state=state)\n",
    "  x = Dense(vocab_size)(x)\n",
    "  return Model(inputs=[seq, state], outputs=x)\n",
    "\n",
    "def Switch():\n",
    "  def func(args):\n",
    "    A, B = args\n",
    "    return tf.cond(K.learning_phase(), lambda: A, lambda: B)\n",
    "  return Lambda(func, name='switch')\n",
    "\n",
    "def recurrent_VAE(sequence, latent_dim):\n",
    "  enc = encoder(sequence, latent_dim)\n",
    "  state, mu, logsig = enc.outputs\n",
    "  # keras Model.__call__(X) connects model.outputs to given X, but does not necessarily\n",
    "  # connect stateful variables such as GRU initial state and batchnorm moving average to X, \n",
    "  # so we explicitly build the model with a \"switch\" input that uses placeholder\n",
    "  # inputs when learning_phase == False (but alas, also always expects dummy PHs)\n",
    "  sequence_ph = Input(shape=(None, int(sequence.shape[-1])))\n",
    "  state_ph = Input(shape=state.shape[1:])\n",
    "  placeholders = [sequence_ph, state_ph]\n",
    "  dec_seq = Switch()([sequence, sequence_ph])\n",
    "  dec_state = Switch()([state, state_ph])\n",
    "  dec = decoder(dec_seq, dec_state, latent_dim)\n",
    "  return Model(inputs=enc.inputs+dec.inputs+placeholders, \n",
    "               outputs=enc.outputs+dec.outputs)\n",
    "\n",
    "def xentropy(X, Xhat):\n",
    "  xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=tf.argmax(X, axis=-1), logits=Xhat)\n",
    "  return tf.reduce_mean(xent)\n",
    "\n",
    "def KL_divergence(mu, logsig):\n",
    "  KL = -0.5 * tf.reduce_sum(1 + logsig - mu**2 - tf.exp(logsig), axis=1)\n",
    "  return tf.reduce_mean(KL)\n",
    "\n",
    "def matrix_to_string(m):\n",
    "  amax = np.argmax(m, axis=-1)\n",
    "  return ''.join([chr(i) for i in amax])\n",
    "\n",
    "def test(sess, Xhat, state, sequence, desired_length):\n",
    "  vocab_size = int(sequence.shape[-1])\n",
    "  latent_dim = int(state.shape[-1])\n",
    "  I = np.eye(vocab_size)\n",
    "  seed = I[[0]]\n",
    "  init_state = np.random.normal(size=latent_dim)\n",
    "  for i in range(desired_length):\n",
    "    feed_dict = {\n",
    "      K.learning_phase(): False,\n",
    "      sequence: [seed],\n",
    "      state: [init_state]}\n",
    "    result = sess.run(Xhat, feed_dict=feed_dict)\n",
    "    new_char = I[[result[0, -1].argmax()]]\n",
    "    seed = np.concatenate((seed, new_char))\n",
    "  return matrix_to_string(seed)\n",
    "\n",
    "def get_text_dataset(pattern, batch_size, length):\n",
    "  txt_files = glob.glob(pattern)\n",
    "  texts = []\n",
    "  for i, fname in enumerate(txt_files):\n",
    "    with open(fname, 'rb') as f:\n",
    "      string = f.read().decode('ascii', 'ignore')\n",
    "      string = ' '.join(string.split())\n",
    "      encoded = string.encode()\n",
    "      texts.append(np.frombuffer(encoded, dtype=np.uint8))\n",
    "  texts = np.concatenate(texts)\n",
    "  texts = texts[texts < 128]\n",
    "  def sample():\n",
    "    while 1:\n",
    "      maxind = texts.size - length\n",
    "      start = np.random.choice(maxind)\n",
    "      stop = start + length\n",
    "      sample = texts[start+1:stop]\n",
    "      sample = np.concatenate([[0], sample], axis=0)\n",
    "      yield sample\n",
    "  dataset = tf.data.Dataset.from_generator(sample, tf.uint8, [max_length])\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(lambda batch: tf.one_hot(batch, 128))\n",
    "  dataset = dataset.prefetch(1)\n",
    "  x = dataset.make_one_shot_iterator().get_next()\n",
    "  return x\n",
    "  \n",
    "def train(model, clip_norm, lambda_r):\n",
    "  X = model.inputs[0]\n",
    "  X_shifted = tf.concat([X[:, 1:], X[:, :1]], axis=1)\n",
    "  sequence_ph, state_ph = model.inputs[-2:]\n",
    "  state, mu, logsig, Xhat = model.outputs\n",
    "  print('got inputs')\n",
    "  XE = xentropy(X_shifted, Xhat)\n",
    "  KL = KL_divergence(mu, logsig)\n",
    "  RG = lambda_r * tf.global_norm(model.trainable_weights)\n",
    "  L = XE + KL + RG\n",
    "  print('got objective')\n",
    "  adam = tf.train.AdamOptimizer()\n",
    "  grad, var = zip(*adam.compute_gradients(\n",
    "    L, var_list=model.trainable_weights))\n",
    "  grad, norm = tf.clip_by_global_norm(grad, clip_norm=clip_norm)\n",
    "  opt = adam.apply_gradients(zip(grad, var)) \n",
    "  print('got optimizer')\n",
    "  dummy_dict = {\n",
    "    K.learning_phase(): True,\n",
    "    sequence_ph: np.zeros((1, 1, int(sequence_ph.shape[-1]))),\n",
    "    state_ph: np.zeros((1, int(state_ph.shape[-1])))}\n",
    "  coord = tf.train.Coordinator()\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('initialized variables')\n",
    "    iteration = 0\n",
    "    while 1:\n",
    "      losses = []\n",
    "      progbar = tqdm.tqdm_notebook(range(100))\n",
    "      for i in progbar:\n",
    "        losses.append(sess.run([opt, L, XE, KL, RG, norm], feed_dict=dummy_dict)[1:])\n",
    "        progbar.set_description('L={:.3f}'.format(losses[-1][0]))\n",
    "        iteration += 1\n",
    "      print('iter: {}'.format(iteration))\n",
    "      print('L:    {}\\nXE:   {}\\nKL:   {}\\nRG:   {}\\ngrad: {}'.format(*np.mean(losses, axis=0)))\n",
    "      print('sample:\\n\\t'+test(sess, Xhat, state_ph, sequence_ph, int(X.shape[1])-1))\n",
    "\n",
    "batch_size = 4096\n",
    "latent_dim = 512\n",
    "max_length = 64\n",
    "clip_norm = 5.0\n",
    "lambda_r = 0.0\n",
    "\n",
    "X = get_text_dataset('/Volumes/1TBSSD/classic-literature-in-ascii/NONFICTION/*.txt', batch_size, max_length)\n",
    "model = recurrent_VAE(X, latent_dim)\n",
    "train(model, clip_norm, lambda_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
